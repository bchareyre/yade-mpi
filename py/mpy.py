


'''
This module defines mpirun(), a parallel implementation of run() using a distributed memory approach. Message passing is done with mpi4py mainly, however some messages are also handled in c++ (with openmpi).


The distribution logic is as follows:
1. Instanciate a complete, ordinary, yade scene
2. Insert subdomains as special yade bodies. This is somehow similar to adding a clump body on the top of clump members
3. Broadcast this scene to all workers. In the initialization phase the workers will:
	- define the bounding box of their assigned bodies and return it to other workers
	- detect which assigned bodies are virtually in interaction with other domains (based on their bounding boxes) and communicate the lists to the relevant workers
	- erase the bodies which are neither assigned nor virtually interacting with the subdomain
4. Run a number of 'regular' iterations without re-running collision detection (verlet dist mechanism)
5. In each regular iteration the workers will:
	- calculate internal and cross-domains interactions
	- execute Newton on assigned bodies (modified Newton skips other domains)
	- send updated positions to other workers and partial force on floor to master

Yet to be implemented is the global update of domain bounds and new collision detection. It could be simply achieved by importing all bodies back in master process and re-running an initialization/distribution but there are certainly mmore efficient techniques to find.

#RULES:
	#- intersections[0] has 0-bodies (to which we need to send force)
	#- intersections[thisDomain] has ids of the other domains overlapping the current ones
	#- intersections[otherDomain] has ids of bodies in _current_ domain which are overlapping with other domain (for which we need to send updated pos/vel)

#HINTS:
- handle subD.intersections with care (same for mirrorIntersections). subD.intersections.append() will not reach the c++ object. subD.intersections can only be assigned (a list of list of int)


'''

import sys
import time
from mpi4py import MPI
import pickle #just for measuring size of message before comm.irecv
import numpy as np

this = sys.modules[__name__]

comm = MPI.COMM_WORLD
rank = comm.Get_rank()
numThreads = comm.Get_size()

ACCUMULATE_FORCES=True #control force summation on master's body. FIXME: if false master goes out of sync since nothing is blocking rank=0 thread
VERBOSE_OUTPUT=False
SEND_SHAPES=False #if false only bodies' states are communicated between threads, else shapes as well (to be implemented)
ERASE_REMOTE = True #erase bodies not interacting wit a given subdomain? else keep dead clones of all bodies in each scene
OPTIMIZE_COM=True
USE_CPP_MPI=True and OPTIMIZE_COM
YADE_TIMING=True #report timing.stats()?


#tags for mpi messages
_SCENE_=11
_SUBDOMAINSIZE_=12
_INTERSECTION_=13
_ID_STATE_SHAPE_=14
_FORCES_=15
_MIRROR_INTERSECTIONS_ = 16
_POS_VEL_ = 17
_BOUNDS_ = 18

#for coloring processes outputs differently
class bcolors:#https://stackoverflow.com/questions/287871/print-in-terminal-with-colors
    HEADER = '\033[95m'
    OKBLUE = '\033[94m'
    OKGREEN = '\033[92m'
    WARNING = '\033[93m'
    FAIL = '\033[91m'
    ENDC = '\033[0m'
    BOLD = '\033[1m'
    UNDERLINE = '\033[4m'

def mprint(m): #this one will print regardless of VERBOSE_OUTPUT
	if 1:
		if rank>0:
			print (bcolors.WARNING if rank==1 else bcolors.OKBLUE) +"worker"+str(rank)+": "+m+bcolors.ENDC
		else: print bcolors.HEADER+"master:" +m+bcolors.ENDC

def wprint(m):
	if not VERBOSE_OUTPUT: return
	mprint(m)

# yadeimport.py is generated by `ln -s yade-versionNo yadeimport.py`, where yade-versionNo is the yade executable
#from yadeimport import yade,sphere,box,Sphere,Body,Subdomain,Bo1_Subdomain_Aabb,typedEngine,PFacet,GridConnection,GridNode,PyRunner,kineticEnergy
from yade import *
from yade.wrapper import *


def receiveForces(subdomains):
	'''
	Accumulate forces from subdomains (only executed by master process), should happen after ForceResetter but before Newton and before any other force-dependent engine (e.g. StressController), could be inserted via yade's pyRunner.
	'''
	if 0: #non-blocking:
		reqForces=[]
		for sd in subdomains:#would be better as a loop on subdomains directly, but we don't have those
			
			#wprint( "master getting forces from "+str(b.subdomain)+"(id="+str(b.id)+")")		
			reqForces.append(comm.irecv(None,sd, tag=_FORCES_))
			#wprint( "master got forces from "+str(b.subdomain)+": "+str(forces))		
		for r in reqForces:
			forces=r.wait()
			for ft in forces:
				#wprint(  "adding force "+str(ft[1])+" to body "+str(ft[0]))
				O.forces.addF(ft[0],ft[1])
				O.forces.addT(ft[0],ft[2])
	else:
		for sd in subdomains:
			forces=comm.recv(source=sd, tag=_FORCES_)
			#wprint( "master got forces from "+str(sd)+": "+str(forces)+" iter="+str(O.iter)+" dt="+str(O.dt))
			for ft in forces:
				#wprint(  "adding force "+str(ft[1])+" to body "+str(ft[0]))
				O.forces.addF(ft[0],ft[1])
				O.forces.addT(ft[0],ft[2])
				
def checkColliderActivated():
	'''
	return true if collision detection needs activation in at least one of them, else false  
	'''
	for sd in range(numThreads):
		if sd==rank: needsCollide = utils.typedEngine("InsertionSortCollider").isActivated()
		else: needsCollide=None
		needsCollide=comm.bcast(needsCollide,root=sd) #collective communication
		if needsCollide:
			if rank==0: mprint("rank "+str(sd)+" triggers collider at iter "+str(O.iter) +": paused")
			return True
	return False


def unboundRemoteBodies():
	'''
	Turn bounding boxes on/off depending on rank
	'''
	for b in O.bodies:# unbound the bodies assigned to workers (not interacting directly with other bodies in master scene)
		if not b.isSubdomain and b.subdomain!=rank:
			b.bounded=False
			b.bound=None
			
def reboundRemoteBodies(ids):
	'''
	update states of bodies handled by other workers, argument 'states' is a list of [id,state] (or [id,state,shape] conditionnaly)
	'''
	for id in ids:
		b = O.bodies[id]
		if not isinstance(b.shape,GridNode): b.bounded=True 

def updateDomainBounds(subdomains): #subdomains is the list of subdomains by body ids
	'''
	Update bounds of current subdomain, broadcast, and receive updated bounds from other subdomains
	Precondition: collider.boundDispatcher.__call__() 
	'''
	wprint( "updating bounds: "+str(subdomains))
	sharedBounds=[None] #initialize with None for master process, to aligned with rank
	buf = []	
	
	for sdId in subdomains:
		if O.bodies[sdId].subdomain!=rank:
			wprint("receiving from "+str(O.bodies[sdId].subdomain))
			buf.append(bytearray(1<<8))
			#comm.isend([subD.boundsMin,subD.boundsMax], dest=worker, tag=_SUBDOMAINSIZE_)
			req=comm.irecv(buf[-1],O.bodies[sdId].subdomain, tag=_SUBDOMAINSIZE_)
			sharedBounds.append(req)
		else: sharedBounds.append(None) #to keep buf aligned with rank
	if rank!=0:#this is not master process, update bounds and share
		subD=O.bodies[subdomains[rank-1]].shape #shorthand to shape of current subdomain
		subD.setMinMax()
		for worker in range(numThreads):
			if worker!=rank:
				wprint("sending "+str([subD.boundsMin,subD.boundsMax]))
				#comm.isend([subD.boundsMin,subD.boundsMax], dest=worker, tag=_SUBDOMAINSIZE_)
				comm.send([subD.boundsMin,subD.boundsMax], dest=worker, tag=_SUBDOMAINSIZE_)
				#sharedBounds.append(req) #keep track of non-blocking messages sent 
	for sdId in subdomains:
		if O.bodies[sdId].subdomain!=rank:
			minmax=sharedBounds[O.bodies[sdId].subdomain].wait()
			#minmax=comm.recv(source=O.bodies[sdId].subdomain, tag=_SUBDOMAINSIZE_)
			wprint( "receiving mn,mx from "+str(O.bodies[sdId].subdomain))
			wprint( "received mn,mx="+str(minmax[0])+" "+str(minmax[1])+" from "+str(O.bodies[sdId].subdomain))
			O.bodies[sdId].shape.boundsMin, O.bodies[sdId].shape.boundsMax = minmax[0],minmax[1]
	wprint( "receiving bounds")	
	#for req in sharedBounds: req.Wait()
	wprint( "bounds updated")
	

def maskedPFacet(pf, boolArray):
	'''
	List bodies within a facet selectively, the ones marked 'True' in boolArray (i.e. already selected from another facet) are discarded
	'''
	l=[]
	for id in [pf.node1.id, pf.node2.id, pf.node3.id, pf.conn1.id, pf.conn2.id, pf.conn3.id]:
		if not boolArray[id]:
			l.append(id)
			boolArray[id]=True

def maskedPFacet(b, boolArray):
	'''
	List bodies within a facet selectively, the ones marked 'True' in boolArray (i.e. already selected from another facet) are discarded
	'''
	l=[]
	pf=b.shape
	for id in [b.id,pf.node1.id, pf.node2.id, pf.node3.id, pf.conn1.id, pf.conn2.id, pf.conn3.id]:
		if not boolArray[id]:
			l.append(id)
			boolArray[id]=True
	return l

def maskedConnection(b, boolArray):
	'''
	List bodies within a facet selectively, the ones marked 'True' in boolArray (i.e. already selected from another facet) are discarded
	'''
	l=[]
	pf=b.shape
	for id in [b.id,pf.node1.id, pf.node2.id]:
		if not boolArray[id]:
			l.append(id)
			boolArray[id]=True
	return l

def genLocalIntersections(subdomains):
	'''
	Defines sets of bodies within current domain overlapping with other domains.
	The structure of the data for domain 'k' is:
	[[id1, id2, ...],  <----------- intersections[0] = ids of bodies in domain k interacting with master domain (subdomain k itself excluded)
	 [id3, id4, ...],  <----------- intersections[1] = ids of bodies in domain k interacting with domain rank=1 (subdomain k itself excluded)
	 ...
	 [domain1, domain2, domain3, ...], <---------- intersections[k] = ranks (not ids!) of external domains interacting with domain k
	 ...
	 ]
	'''
	intersections=[[] for n in range(numThreads)]
	for sdId in subdomains:
		#grid nodes or grid connections could be appended twice or more, as they can participate in multiple pfacets and connexions
		#this bool list is used to append only once
		appended = np.repeat([False],len(O.bodies))
		subdIdx=O.bodies[sdId].subdomain
		intrs=O.interactions.withBodyAll(sdId)
		#special case when we get interactions with current domain, only used to define interactions with master, otherwise some intersections would appear twice
		if subdIdx==rank:
			for i in intrs:
				otherId=i.id1 if i.id2==sdId else i.id2
				b=O.bodies[otherId]
				if b.subdomain==0:
					if isinstance(b.shape,PFacet):
						intersections[0]+= maskedPFacet(b, appended); continue
					if isinstance(b.shape,GridConnection):
						intersections[0]+=maskedConnection(b, appended); continue
					#else (standalone body, normal case)
					intersections[0].append(otherId)
			if len(intersections[0])>0: intersections[subdIdx].append(0)
			continue
		# normal case
		for i in intrs:
			otherId=i.id1 if i.id2==sdId else i.id2
			b=O.bodies[otherId]
			if b.subdomain!=rank: continue
			if b.isSubdomain: intersections[rank].append(subdIdx) #intersecting subdomain (will need to receive updated positions from there)
			else:
				if isinstance(b.shape,PFacet):
						intersections[subdIdx]+= maskedPFacet(b, appended); continue
				if isinstance(b.shape,GridConnection):
						intersections[subdIdx]+=maskedConnection(b, appended); continue
				#else (standalone body, normal case)
				intersections[subdIdx].append(otherId)

		#for master domain set list of interacting subdomains (could be handled above but for the sake of clarity complex if-else-if are avoided for now)
		if rank==0 and len(intersections[subdIdx])>0:
			intersections[0].append(subdIdx)
	#wprint( "found "+str(len(intrs))+" intersections"+str(intersections))
	return intersections

def updateRemoteStates(states, setBounded=False):
	'''
	update states of bodies handled by other workers, argument 'states' is a list of [id,state] (or [id,state,shape] conditionnaly)
	'''
	ids=[]
	for bst in states:
		#print bst[0],O.bodies[bst[0]]
		ids.append(bst[0])
		b=O.bodies[bst[0]]
		b.state=bst[1]
		#if SEND_SHAPES: b.shape=bst[2]
		if setBounded and not isinstance(b.shape,GridNode): b.bounded=True 
	return ids

def genUpdatedStates(b_ids):
	'''
	return list of [id,state] (or [id,state,shape] conditionnaly) to be sent to other workers
	'''
	return [[id,O.bodies[id].state] for id in b_ids] if not SEND_SHAPES else [[id,O.bodies[id].state,O.bodies[id].shape] for id in b_ids]



#############   COMMUNICATIONS   ################"

def sendRecvStates():
	#____1. get ready to receive positions from other subdomains
	
	pstates = []
	buf = [] #heuristic guess, assuming number of intersecting is ~linear in the number of rows, needs
		
	if rank!=0: #the master process never receive updated states (except when gathering)
		for otherDomain in O.subD.intersections[rank]:
			#wprint( str(": getting states from ")+str(otherDomain))
			if not USE_CPP_MPI:
				buf.append(bytearray(1<<22)) #FIXME: smarter size? this is for a few thousands states max (empirical); bytearray(1<<24) = 128 MB 
				pstates.append( comm.irecv(buf[-1],otherDomain, tag=_ID_STATE_SHAPE_))  #warning leaving buffer size undefined crash for large subdomains (MPI_ERR_TRUNCATE: message truncated)
			else:
				O.subD.mpiIrecvStates(otherDomain) #use yade's messages (coded in cpp)
		#mprint("prepared receive: "+str(time.time()-start)); start=time.time()
	
	#____2. broadcast new positions (should be non-blocking if n>2, else lock) - this includes subdomain bodies intersecting the current one	
	reqs=[]
	for k in O.subD.intersections[rank]:
		if k==rank or k==0: continue #don't broadcast to itself... OTOH this list intersections[rank] will be used to receive
		#if len(b_ids)>0:#skip empty intersections, it means even the bounding boxes of the corresponding subdomains do not overlap
		#mprint("sending "+str(len(O.subD.intersections[k]))+" states to "+str(k))
		if not OPTIMIZE_COM:
			comm.send(genUpdatedStates(O.subD.intersections[k]), dest=k, tag=_ID_STATE_SHAPE_) #should be non-blocking if n>2, else lock?
		else:
			if not USE_CPP_MPI:
				reqs.append(comm.isend(subD.getStateValues(k), dest=k, tag=_ID_STATE_SHAPE_)) #should be non-blocking if n>2, else lock?
			else:
				O.subD.mpiSendStates(k)
		for r in reqs: r.wait() #empty if USE_CPP_MPI
		
	#____3. receive positions and update bodies
	
	if rank==0: return #positions sent from master, done. Will receive forces instead of states
	if not USE_CPP_MPI:
		nn=0	
		for ss in pstates:
			states=ss.wait()
			if not OPTIMIZE_COM:
				updateRemoteStates(states)
			else:
				O.subD.setStateValuesFromIds(O.subD.mirrorIntersections[O.subD.intersections[rank][nn]],states)
				nn+=1
	else:
		for otherDomain in O.subD.intersections[rank]:
			#mprint("listen cpp from "+str(otherDomain)+" in "+str(O.subD.intersections[rank]))
			#subD.mpiRecvStates(otherDomain)
			O.subD.mpiWaitReceived(otherDomain)
			O.subD.setStateValuesFromBuffer(otherDomain)


def isendRecvForces():
	'''
	Communicate forces from subdomain to master
	Warning: the sending sides (everyone but master) must wait() the returned list of requests
	'''	
	O.freqs=[] #keep that one defined even if empty, it is accessed in other functions
	if ACCUMULATE_FORCES:
		if rank!=0:
			forces0=[[id,O.forces.f(id),O.forces.t(id)] for id in  O.subD.mirrorIntersections[0]]
			#wprint ("worker "+str(rank)+": sending "+str(len(forces0))+" "+str("forces to 0 "))
			#O.freqs.append(comm.isend(forces0, dest=0, tag=_FORCES_))
			comm.send(forces0, dest=0, tag=_FORCES_)
		else: #master
			receiveForces(O.subD.intersections[0])

def waitForces():
	'''
	wait until all forces are sent to master. 
	O.freqs is empty for master, and for all threads if not ACCUMULATE_FORCES
	'''
	for r in O.freqs: r.wait()


##### INITIALIZE MPI #########

# Flag used after import of this module, turned True after scene is distributed
O.splitted=False
O.splittedOnce=False #after the first split we have additional bodies (Subdomains) and engines in the merged scene, use this flag to know

def mergeScene():
	if O.splitted:
		if rank>0:
			comm.send(O.subD.getStateValuesFromIds([b.id for b in O.bodies if b.subdomain==rank]), dest=0, tag=_POS_VEL_)
			comm.send([b.bound for b in O.bodies if b.subdomain==rank], dest=0, tag=_BOUNDS_) #optional, for collider.targetInter>0
		else: #master
			for worker in range(1,numThreads):
				# get pos/vel
				dat = comm.recv(source=worker,tag=_POS_VEL_)
				bounds = comm.recv(source=worker,tag=_BOUNDS_) #optional
				# generate corresponding ids (order is the same for both master and worker)
				ids = [b.id for b in O.bodies if b.subdomain==worker]
				O.subD.setStateValuesFromIds(ids,dat)
				for k in range(len(ids)):
					O.bodies[ids[k]].bound = bounds[k]
				reboundRemoteBodies(ids)
		# turn mpi engines off
		sendRecvStatesRunner.dead = isendRecvForcesRunner.dead = waitForcesRunner.dead = True
		O.splitted=False
		collider.doSort = True
		#O.save('mergedScene.yade')


def splitScene():
	'''
	Split a monolithic scene into distributed scenes on threads
	precondition: the bodies have subdomain no. set in user script
	'''
	if rank == 0:
		
		if not O.splittedOnce: 
			O._sceneObj.subdomain=0
			O.subD=Subdomain() #for storage only, this one will not be used beyond that 
			subD= O.subD #alias
			#insert "meta"-bodies
			subD.subdomains=[] #list subdomains by body ids
			
			for k in range(1,numThreads):
				domainBody=Body(shape=Subdomain(ids=[b.id for b in O.bodies if b.subdomain==k]),subdomain=k) #note: not clear yet how shape.subDomainIndex and body.subdomain should interact, currently equal values
				domainBody.isSubdomain=True
				subD.subdomains.append(O.bodies.append(domainBody))

			#tell the collider how to handle this new thing
			collider.boundDispatcher.functors=collider.boundDispatcher.functors+[Bo1_Subdomain_Aabb()]
			collider.targetInterv=0
			#collider.verletDist = 0.01 #FIXME: in general yade set this one automatically but here it will only happen after collider.__call__() (i.e too late...)
			
			#BEGIN Garbage (should go to some init(), usually done in collider.__call__() but in the mpi case we want to collider.boundDispatcher.__call__() before collider.__call__()
			collider.boundDispatcher.sweepDist=collider.verletDist;
			collider.boundDispatcher.minSweepDistFactor=collider.minSweepDistFactor;
			collider.boundDispatcher.targetInterv=collider.targetInterv;
			collider.boundDispatcher.updatingDispFactor=collider.updatingDispFactor;
			#END Garbage
		
		#distribute work
		for worker in range(1,numThreads):
			sceneAsString=O.sceneToString()
			comm.send(sceneAsString, dest=worker, tag=_SCENE_) #sent with scene.subdomain=1, better make subdomain index a passed value so we could pass the sae string to every worker (less serialization+deserialization)
			
	else:
		O.stringToScene(comm.recv(source=0, tag=_SCENE_)) #receive a scene pre-processed by master (i.e. with appropriate body.subdomain's)  
		#print "worker 1 received",len(O.bodies),"bodies (verletDist=",collider.verletDist,")"
		O._sceneObj.subdomain = rank
		
		
		domainBody=None
		subdomains=[] #list of subdomains by body id
		for b in O.bodies:
			if b.isSubdomain:
				subdomains.append(b.id)
				if b.subdomain==rank: domainBody=b
			
		if domainBody==None: print "SUBDOMAIN NOT FOUND FOR RANK=",rank
		O.subD = domainBody.shape
		O.subD.subdomains = subdomains
		#subD= O._sceneObj.subD #alias
	subD = O.subD #alias
	
	#update bounds wrt. updated subdomain(s) min/max and unbounded bodies
	unboundRemoteBodies()
	collider.boundDispatcher.__call__()
	updateDomainBounds(subD.subdomains) #triggers communications
	collider.__call__() #see [1]
	subD.intersections=genLocalIntersections(subD.subdomains)
	#O._sceneObj.doSort = True #refresh 

	#O._sceneObj.doSort= True
	
	
	#update mirror intersections so we know message sizes in advance
	subD.mirrorIntersections=[[] for n in range(numThreads)]
	if rank==0:#master domain
		
		for worker in range(1,numThreads):#FIXME: we actually don't need so much data since at this stage the states are unchanged and the list is used to re-bound intersecting bodies, this is only done in the initialization phase, though
			#states= [[id,O.bodies[id].state,O.bodies[id].shape] for id in intersections[worker]]
			wprint("sending mirror intersections to "+str(worker)+" ("+str(len(subD.intersections[worker]))+" bodies)")
			comm.send(subD.intersections[worker], dest=worker, tag=_MIRROR_INTERSECTIONS_)
			#comm.send(states, dest=worker, tag=_ID_STATE_SHAPE_) #sent with scene.subdomain=1
	else:
		# from master
		b_ids=comm.recv(source=0, tag=_MIRROR_INTERSECTIONS_)
		if len(b_ids)>0:
			reboundRemoteBodies(b_ids)
			subD.mirrorIntersections= [b_ids]+subD.mirrorIntersections[1:]
			# since interaction with 0-bodies couldn't be detected before, mirror intersections from master will
			# tell if we need to wait messages from master (and this is declared via intersections) 
			if not 0 in subD.intersections[rank]:
				temp=subD.intersections[rank]
				temp+=[0]
				subD.intersections=subD.intersections[:rank]+[temp]+subD.intersections[rank+1:]
			else:
				mprint("0 already in intersections (should not happen)")
		reqs=[]
		
		#from workers
		for worker in subD.intersections[rank]:
			if worker==0: continue #already received above
			#wprint("subD.intersections["+str(rank)+"]: "+str(subD.intersections[rank]))
			buf = bytearray(1<<22) #CRITICAL
			reqs.append([worker,comm.irecv(buf, worker, tag=_MIRROR_INTERSECTIONS_)])

		for worker in subD.intersections[rank]:
			if worker==0: continue #we do not send positions to master, only forces
			#wprint("sending "+str(len(subD.intersections[worker]))+" states to "+str(worker))
			comm.send(subD.intersections[worker], dest=worker, tag=_MIRROR_INTERSECTIONS_)
			#wprint("sent")

		nn=0
		for req in reqs:
			if subD.intersections[rank][nn]==0: nn+=1
			sd=subD.intersections[rank][nn]
			nn+=1
			#states=pstates.wait()
			#wprint("received "+str(len(states))+ " states")
			#ids=updateRemoteStates(states,True)
			intrs=req[1].wait()
			subD.mirrorIntersections= subD.mirrorIntersections[0:req[0]]+[intrs]+subD.mirrorIntersections[req[0]+1:]
			reboundRemoteBodies(intrs)
			
	if ERASE_REMOTE and rank>0: #workers suppress external bodies from scene, master will keep all bodies anyway 
		numBodies = len(O.bodies)
		for id in range(numBodies):
			if not O.bodies[id].bounded and O.bodies[id].subdomain!=rank:
				connected = False #a gridNode could be needed as part of interacting facet/connection even if not overlaping a specific subdomain. Assume connections are always bounded for now, we thus only check nodes.
				if isinstance(O.bodies[id].shape,GridNode):
					for f in O.bodies[id].shape.getPFacets():
						if f.bounded: connected = True
					for c in O.bodies[id].shape.getConnections():
						if c.bounded: connected = True
				if not connected: O.bodies.erase(id)
	#collider.doSort = True
	collider.__call__()
				
	idx = O.engines.index(utils.typedEngine("NewtonIntegrator"))
	moduleName = "mp"
	if not O.splittedOnce: 
		# append states communicator after Newton
		O.engines=O.engines[:idx+1]+[PyRunner(iterPeriod=1,initRun=True,command="sys.modules['yade.mpy'].sendRecvStates()",label="sendRecvStatesRunner")]+O.engines[idx+1:]
		
		# append force communicator before Newton
		O.engines=O.engines[:idx]+[PyRunner(iterPeriod=1,initRun=True,command="sys.modules['yade.mpy'].isendRecvForces()",label="isendRecvForcesRunner")]+O.engines[idx:]
		
		# append engine waiting until forces are effectively sent to master
		O.engines=O.engines+[PyRunner(iterPeriod=1,initRun=True,command="sys.modules['yade.mpy'].waitForces()",label="waitForcesRunner")]
		O.engines=O.engines+[PyRunner(iterPeriod=1,initRun=True,command="if sys.modules['yade.mpy'].checkColliderActivated(): O.pause()",label="collisionChecker")]
	else:
		sendRecvStatesRunner.dead = isendRecvForcesRunner.dead = waitForcesRunner.dead = False
	
	# mark scene splitted
	O.splitted=True
	O.splittedOnce=True


##### RUN MPI #########
def mpirun(nSteps,mergeSplit=False):
	initStep = O.iter
	if not O.splitted: splitScene()
	if YADE_TIMING:
		O.timingEnabled=True
	if not mergeSplit: #run until collider is activated then stop		
		O.run(nSteps,True)
	else: #merge/split for each collider update
		collisionChecker.dead=True
		while (O.iter-initStep)<nSteps:
			O.step()
			if checkColliderActivated() and (O.iter-initStep)<nSteps:
				mergeScene()
				splitScene()

			
	if YADE_TIMING:
		from yade import timing
		time.sleep((numThreads-rank)*0.1) #avoid mixing the final output, timing.stats() is independent of the sleep
		mprint( "#####  Worker "+str(rank)+"  ######")
		timing.stats() #specific numbers for -n4 and gabion.py

